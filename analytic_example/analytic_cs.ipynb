{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91588d1e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e1448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import casadi as cd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775f1f08",
   "metadata": {},
   "source": [
    "You can adapt all plotting parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435ce4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    \"font.size\": 12,\n",
    "    # \"text.usetex\": True,\n",
    "    \"text.usetex\": False,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a690c90d",
   "metadata": {},
   "source": [
    "# Calculation of the policy gradient and Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d2798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the normal distribution of noise w\n",
    "mu_w = 0.0 # Mean\n",
    "sigma_w = cd.SX.sym(\"sigma_w\") # Standard deviation\n",
    "\n",
    "# Parameters for the normal distribution of initial state s0\n",
    "mu_s0 = 0.0 # Mean\n",
    "sigma_s0 = cd.SX.sym(\"sigma_s0\") # Standard deviation\n",
    "\n",
    "\n",
    "# States and actions\n",
    "s = cd.SX.sym(\"s\") # State\n",
    "a = cd.SX.sym(\"a\") # Action\n",
    "\n",
    "# Parameters \n",
    "gamma = cd.SX.sym(\"gamma\") # Discount factor\n",
    "\n",
    "# Policy\n",
    "theta = cd.SX.sym(\"theta\") # Policy parameter\n",
    "pi = - theta ** 2 * s # Policy function\n",
    "jac_pi_theta = cd.jacobian(pi, theta) # Jacobian of the policy with respect to theta\n",
    "jac2_pi_theta = cd.jacobian(jac_pi_theta, theta) # Second derivative of the policy with respect to theta\n",
    "\n",
    "# Reward function\n",
    "reward = -0.5 * (s ** 2 + a ** 2)\n",
    "reward_func = cd.Function(\"reward\", [s, a], [reward])\n",
    "reward_at_policy = reward_func(s, pi)\n",
    "\n",
    "# Value function parameters\n",
    "p = -1/2 * (1 + theta ** 4) / (1 - gamma * (1 - theta ** 2) ** 2)\n",
    "q = gamma/(1 - gamma) * sigma_w ** 2 * p\n",
    "\n",
    "# Derivatives of p and q with respect to theta\n",
    "grad_p_theta = cd.jacobian(p, theta)\n",
    "grad_q_theta = cd.jacobian(q, theta)\n",
    "\n",
    "# Value and Q functions\n",
    "V = p * s ** 2 + q\n",
    "V_func = cd.Function(\"V\", [s, theta, gamma, sigma_w], [V], [\"s\", \"theta\", \"gamma\", \"sigma_w\"], [\"V\"])\n",
    "Q = reward + gamma * (q + p * sigma_w ** 2) + gamma * p * (s + a) ** 2\n",
    "Q_func = cd.Function(\"Q\", [s, a, theta, gamma, sigma_w], [Q], [\"s\", \"a\", \"theta\", \"gamma\", \"sigma_w\"], [\"Q\"])\n",
    "\n",
    "# Expected cumulative reward\n",
    "J = q + p * sigma_s0 ** 2\n",
    "J_func = cd.Function(\"J\", [theta, gamma, sigma_w, sigma_s0], [J], [\"theta\", \"gamma\", \"sigma_w\", \"sigma_s0\"], [\"J\"])\n",
    "\n",
    "# Derivatives of J with respect to theta (these are the exact derivatives)\n",
    "hess_J, grad_J = cd.hessian(J, theta)\n",
    "grad_J_func = cd.Function(\"grad_J\", [theta, gamma, sigma_w, sigma_s0], [grad_J], [\"theta\", \"gamma\", \"sigma_w\", \"sigma_s0\"], [\"grad_J\"])\n",
    "hess_J_func = cd.Function(\"hess_J\", [theta, gamma, sigma_w, sigma_s0], [hess_J], [\"theta\", \"gamma\", \"sigma_w\", \"sigma_s0\"], [\"hess_J\"])\n",
    "\n",
    "# Derivatives of the Q function with respect to a\n",
    "hess_Q_a, grad_Q_a = cd.hessian(Q, a)\n",
    "\n",
    "grad_Q_a_func = cd.Function(\"grad_Q_a\", [s, a, theta, gamma], [grad_Q_a])\n",
    "grad_Q_a_at_policy = grad_Q_a_func(s, pi, theta, gamma)\n",
    "grad_Q_a_at_policy_func = cd.Function(\"grad_Q_a_at_policy\", [s, theta, gamma], [grad_Q_a_at_policy])\n",
    "\n",
    "hess_Q_a_func = cd.Function(\"hess_Q_a\", [s, a, theta, gamma], [hess_Q_a])\n",
    "hess_Q_a_at_policy = hess_Q_a_func(s, pi, theta, gamma)\n",
    "hess_Q_a_at_policy_func = cd.Function(\"hess_Q_a_at_policy\", [s, theta, gamma], [hess_Q_a_at_policy])\n",
    "\n",
    "# Auxiliary variables to save typing effort\n",
    "c1 = 2 * theta ** 3 * (2 * gamma * p - 1) - 4 * gamma * p * theta\n",
    "\n",
    "# Expected value of squared state\n",
    "expected_value_of_squared_s = (grad_q_theta + sigma_s0 ** 2 * grad_p_theta) / c1\n",
    "\n",
    "# Gradient of the expected cumulative reward but using the deterministic policy theorem\n",
    "grad_J_RL = c1 * expected_value_of_squared_s\n",
    "grad_J_RL_func = cd.Function(\"grad_J_RL\", [theta, gamma, sigma_w, sigma_s0], [grad_J_RL], [\"theta\", \"gamma\", \"sigma_w\", \"sigma_s0\"], [\"grad_J_RL\"])\n",
    "\n",
    "# Gauss-Newton matrix (M2)\n",
    "Gauss_Newton_matrix = 4 * theta ** 2 * (2 * gamma * p - 1) * expected_value_of_squared_s\n",
    "Gauss_Newton_matrix_func = cd.Function(\"Gauss_Newton_matrix\", [theta, gamma, sigma_w, sigma_s0], [Gauss_Newton_matrix], [\"theta\", \"gamma\", \"sigma_w\", \"sigma_s0\"], [\"Gauss_Newton_matrix\"])\n",
    "\n",
    "# Approximate Hessian (M1 + M2)\n",
    "H1 = (2 * theta ** 2 *(2 * gamma * p - 1)- 4 * gamma * p) * expected_value_of_squared_s\n",
    "H2 = Gauss_Newton_matrix\n",
    "H = H1 + H2\n",
    "H_func = cd.Function(\"H\", [theta, gamma, sigma_w, sigma_s0], [H], [\"theta\", \"gamma\", \"sigma_w\", \"sigma_s0\"], [\"H\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0ef783",
   "metadata": {},
   "source": [
    "# Plotting\n",
    "\n",
    "## 1. Expected cumulative reward and the gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9829ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define range of theta values for plotting\n",
    "theta_num = np.linspace(0.4, 0.9, 1000)\n",
    "\n",
    "# Set fixed parameters for the environment\n",
    "gamma_num = 0.9\n",
    "sigma_w_num = 0.1 ** 0.5\n",
    "sigma_s0_num = 0.1 ** 0.5\n",
    "\n",
    "# Prepare rootfinder dictionary for finding optimal theta\n",
    "rf_dict = {\n",
    "    \"g\": grad_J,  # Gradient of J with respect to theta (exact)\n",
    "    \"x\": theta,   # Variable to solve for (theta^\\star)\n",
    "    \"p\": cd.vertcat(*[gamma, sigma_w, sigma_s0]),  # Parameters\n",
    "}\n",
    "\n",
    "# Create rootfinder object using Newton's method\n",
    "rf_theta_star = cd.rootfinder(\"grad_J\", \"newton\", rf_dict)\n",
    "\n",
    "# Find optimal theta (where grad_J = 0)\n",
    "theta_star = rf_theta_star(x0 = cd.DM(0.75), p = cd.DM([gamma_num, sigma_w_num, sigma_s0_num]))[\"x\"]\n",
    "\n",
    "# Evaluate expected cumulative reward and its gradient for plotting\n",
    "# Here we use the exact gradient and the Gauss-Newton matrix\n",
    "J_num = J_func(theta_num, gamma_num, sigma_w_num, sigma_s0_num)\n",
    "J_star = J_func(theta_star, gamma_num, sigma_w_num, sigma_s0_num)\n",
    "\n",
    "# Evaluate the gradient of J via the deterministic policy gradient theorem\n",
    "grad_J_num = grad_J_func(theta_num, gamma_num, sigma_w_num, sigma_s0_num)\n",
    "grad_J_RL_num = grad_J_RL_func(theta_num, gamma_num, sigma_w_num, sigma_s0_num)\n",
    "grad_J_RL_star = grad_J_RL_func(theta_star, gamma_num, sigma_w_num, sigma_s0_num)\n",
    "\n",
    "# Plot results: J(theta) and gradients\n",
    "nrows = 2\n",
    "fig, ax = plt.subplots(nrows = nrows, figsize=(5, 4 * nrows), sharex=True, constrained_layout=True)\n",
    "\n",
    "# Plot expected cumulative reward\n",
    "ax[0].plot(theta_num, J_num, color='blue', label =r\"$J(\\theta)$\")\n",
    "ax[0].plot(theta_star, J_star, marker = \"*\", markersize = 10, linestyle = \"None\", color='red', label = r\"$J(\\theta^\\star)$\")\n",
    "ax[0].set_ylabel(r\"$J(\\theta)$\")\n",
    "ax[0].set_xlim(min(theta_num), max(theta_num))\n",
    "\n",
    "# Plot gradients\n",
    "ax[1].plot(theta_num, grad_J_num, color='blue', label=r\"$\\nabla J(\\theta)$\")\n",
    "ax[1].plot(theta_num, grad_J_RL_num, color='red', linestyle = \"dashed\", label=r\"$\\mathbb{E}_{s}[\\nabla_{\\theta}\\pi_\\theta(s) \\nabla Q(s, a)\\vert_{a=\\pi_{\\theta}(s)}]$\")\n",
    "ax[1].plot(theta_star, grad_J_RL_star, marker = \"*\", markersize = 10, color='red', linestyle = \"None\", label = r\"$J(\\theta^\\star)$\")\n",
    "\n",
    "ax[1].set_ylabel(r\"$\\nabla J(\\theta)$\")\n",
    "ax[1].legend()\n",
    "\n",
    "# Add grid to all subplots\n",
    "for axis in ax:\n",
    "    axis.grid()\n",
    "\n",
    "ax[-1].set_xlabel(r\"$\\theta$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441a57e2",
   "metadata": {},
   "source": [
    "## 2. Hessians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601497f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the exact Hessian, approximate Hessian, and Gauss-Newton matrix for a range of theta values\n",
    "hess_J_num = hess_J_func(theta_num, gamma_num, sigma_w_num, sigma_s0_num) # This is the exact Hessian\n",
    "hess_J_star = hess_J_func(theta_star, gamma_num, sigma_w_num, sigma_s0_num)\n",
    "\n",
    "H_num = H_func(theta_num, gamma_num, sigma_w_num, sigma_s0_num) # This is the approximate Hessian (M1 + M2)\n",
    "H_star = H_func(theta_star, gamma_num, sigma_w_num, sigma_s0_num)\n",
    "\n",
    "Gauss_Newton_matrix_num = Gauss_Newton_matrix_func(theta_num, gamma_num, sigma_w_num, sigma_s0_num) # This is the Gauss-Newton matrix (M2)\n",
    "Gauss_Newton_matrix_star = Gauss_Newton_matrix_func(theta_star, gamma_num, sigma_w_num, sigma_s0_num)\n",
    "\n",
    "# Plot the Hessians and Gauss-Newton matrix\n",
    "k = 1\n",
    "fig, ax = plt.subplots(1, 1, figsize = (k * 5, k* 3.5), constrained_layout=True)\n",
    "\n",
    "# Plot the exact Hessian, the approximate Hessian (M1 + M2), and the Gauss-Newton matrix (M2)\n",
    "ax.plot(theta_num, hess_J_num, linestyle = \"solid\", label=r\"Exact Hessian $\\nabla_{\\theta}^2 J(\\theta)$\", color=\"tab:red\")\n",
    "ax.plot(theta_num, H_num, linestyle = \"dashed\", label=r\"Approx. Hessian $M_1(\\theta) + M_2(\\theta)$\", color=\"tab:blue\")\n",
    "ax.plot(theta_num, Gauss_Newton_matrix_num, linestyle = \"-.\", label=r\"Gauss-Newton $M_2(\\theta)$ (Proposed)\", color=\"tab:orange\")\n",
    "\n",
    "# Mark the value at the optimal parameter theta*\n",
    "ax.plot(theta_star, hess_J_star, marker = \"*\", markersize = 15, linestyle = \"None\", color='red', label = r\"At optimal parameter $\\theta^\\star$\")\n",
    "\n",
    "# Set axis labels and limits\n",
    "ax.set_xlabel(r\"$\\theta$\")\n",
    "ax.set_ylabel(r\"$\\nabla_{\\theta}^2 J(\\theta)$\")\n",
    "ax.grid()\n",
    "ax.set_xlim(min(theta_num), max(theta_num))\n",
    "ax.set_ylim([-25, 3])\n",
    "\n",
    "# Move the legend inside the plot to avoid it being cut off\n",
    "ax.legend(loc=\"lower right\", ncol=1, fontsize=11.5, frameon=True)\n",
    "\n",
    "# Save the figure in high resolution\n",
    "fig.savefig(\"Hessian_investigation.png\", bbox_inches='tight', dpi=1200.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d3b9e",
   "metadata": {},
   "source": [
    "# Optimization with Newton, approx. Newton, Gauss-Newton, and Gradient ascent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be82180",
   "metadata": {},
   "source": [
    "## 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8685d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial value for theta\n",
    "theta_init = 0.6\n",
    "\n",
    "# Number of optimization steps\n",
    "n_steps = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db40021",
   "metadata": {},
   "source": [
    "## 2. Newton's method with exact Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f5dd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Newton's method with exact Hessian ---\n",
    "alpha_exact_hessian = 1.0\n",
    "theta_init_full_hessian = theta_init\n",
    "theta_full_hessian_list = [theta_init_full_hessian]\n",
    "\n",
    "for idx in range(n_steps):\n",
    "    # Compute gradient and Hessian of J at current theta\n",
    "    grad_J_full_hessian = grad_J_func(theta_init_full_hessian, gamma_num, sigma_w_num, sigma_s0_num)\n",
    "    hess_J_full_hessian = hess_J_func(theta_init_full_hessian, gamma_num, sigma_w_num, sigma_s0_num)\n",
    "\n",
    "    # Newton update: theta <- theta - H^{-1} * grad\n",
    "    theta_init_full_hessian -= alpha_exact_hessian * np.linalg.inv(hess_J_full_hessian) @ grad_J_full_hessian\n",
    "\n",
    "    # Store updated theta\n",
    "    theta_full_hessian_list.append(theta_init_full_hessian[0, 0])\n",
    "\n",
    "# Stack results for plotting\n",
    "theta_full_hessian = np.vstack(theta_full_hessian_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9449d2bf",
   "metadata": {},
   "source": [
    "## 3. Newton's method with approximate Hessian ($M_1 + M_2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Newton's method with approximate Hessian (M1 + M2) ---\n",
    "alpha_approx_hessian = 1.0\n",
    "theta_init_approx_newton = theta_init\n",
    "theta_approx_newton_list = [theta_init_approx_newton]\n",
    "\n",
    "for idx in range(n_steps):\n",
    "    # Compute gradient and approximate Hessian of J\n",
    "    grad_J_approx_newton = grad_J_func(theta_init_approx_newton, gamma_num, sigma_w_num, sigma_s0_num)\n",
    "    H_approx_newton = H_func(theta_init_approx_newton, gamma_num, sigma_w_num, sigma_s0_num)\n",
    "\n",
    "    # Newton update with approximate Hessian\n",
    "    theta_init_approx_newton -= alpha_approx_hessian * np.linalg.inv(H_approx_newton) @ grad_J_approx_newton\n",
    "\n",
    "    # Store updated theta\n",
    "    theta_approx_newton_list.append(theta_init_approx_newton[0, 0])\n",
    "\n",
    "theta_approx_newton = np.vstack(theta_approx_newton_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df95c58e",
   "metadata": {},
   "source": [
    "## 4. Newton's method with Gauss-Newton approximation ($M_2$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4063f42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gauss-Newton method (M2 only) ---\n",
    "alpha_Gauss_Newton = 1.0\n",
    "theta_init_Gauss_Newton = theta_init\n",
    "theta_Gauss_Newton_list = [theta_init_Gauss_Newton]\n",
    "\n",
    "for idx in range(n_steps):\n",
    "    # Compute gradient and Gauss-Newton matrix\n",
    "    grad_J_Gauss_Newton = grad_J_func(theta_init_Gauss_Newton, gamma_num, sigma_w_num, sigma_s0_num)\n",
    "    Gauss_Newton_matrix = Gauss_Newton_matrix_func(theta_init_Gauss_Newton, gamma_num, sigma_w_num, sigma_s0_num)\n",
    "\n",
    "    # Newton update with Gauss-Newton matrix\n",
    "    theta_init_Gauss_Newton -= alpha_Gauss_Newton * np.linalg.inv(Gauss_Newton_matrix) @ grad_J_Gauss_Newton\n",
    "\n",
    "    # Store updated theta\n",
    "    theta_Gauss_Newton_list.append(theta_init_Gauss_Newton[0, 0])\n",
    "\n",
    "theta_Gauss_Newton = np.vstack(theta_Gauss_Newton_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e5551c",
   "metadata": {},
   "source": [
    "# 5. Gradient ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50654b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Gradient ascent ---\n",
    "alpha_gradient_ascent = 0.1\n",
    "theta_init_gradient_ascent = theta_init\n",
    "theta_gradient_ascent_list = [theta_init_gradient_ascent]\n",
    "\n",
    "for idx in range(n_steps):\n",
    "    # Compute gradient of J\n",
    "    grad_J_gradient_ascent = grad_J_func(theta_init_gradient_ascent, gamma_num, sigma_w_num, sigma_s0_num)\n",
    "\n",
    "    # Gradient ascent update: theta <- theta + alpha * grad\n",
    "    theta_init_gradient_ascent += alpha_gradient_ascent * grad_J_gradient_ascent\n",
    "\n",
    "    # Store updated theta\n",
    "    theta_gradient_ascent_list.append(theta_init_gradient_ascent[0, 0])\n",
    "\n",
    "theta_gradient_ascent = np.vstack(theta_gradient_ascent_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c5e32",
   "metadata": {},
   "source": [
    "# Plot the error dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abee288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "\n",
    "# Compute absolute errors between iterates and optimal theta for each method\n",
    "full_hessian_error = np.abs(theta_full_hessian - theta_star)\n",
    "approx_newton_error = np.abs(theta_approx_newton - theta_star)\n",
    "Gauss_Newton_error = np.abs(theta_Gauss_Newton - theta_star)\n",
    "gradient_ascent_error = np.abs(theta_gradient_ascent - theta_star)\n",
    "\n",
    "# Plot convergence errors for all methods\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 3.5), constrained_layout=True)\n",
    "\n",
    "# Plot error for Newton's method with exact Hessian, approximate Hessian, Gauss-Newton method, and gradient ascent \n",
    "ax.plot(np.arange(n_steps + 1), full_hessian_error, label=r\"Exact Hessian $\\nabla_{\\theta}^2J(\\theta)$\", color=\"tab:red\")\n",
    "ax.plot(np.arange(n_steps + 1), approx_newton_error, label=r\"Approx. Hessian $M_1(\\theta) + M_2(\\theta)$\", color=\"tab:blue\", linestyle=\"dashed\")\n",
    "ax.plot(np.arange(n_steps + 1), Gauss_Newton_error, label=r\"Gauss-Newton $M_2(\\theta)$ (Proposed)\", color=\"tab:orange\", linestyle =\"-.\")\n",
    "ax.plot(np.arange(n_steps + 1), gradient_ascent_error, label=\"Gradient ascent\", color=\"tab:green\", linestyle=\"dotted\")\n",
    "\n",
    "# Set axis labels and formatting\n",
    "ax.set_xlabel(r\"Iteration $k$\")\n",
    "ax.set_ylabel(r\"$| \\theta_k - \\theta^\\star|$\")\n",
    "ax.set_yscale(\"log\")  # Log scale for error\n",
    "ax.set_ylim([1e-10, 1e0])\n",
    "ax.set_xlim(0, n_steps)\n",
    "ax.legend(loc=\"upper right\", ncol=1, fontsize=11.5, frameon=True)\n",
    "ax.grid()\n",
    "ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))  # Show integer ticks only\n",
    "\n",
    "# Save the figure as a high-resolution PNG\n",
    "fig.savefig(\"theta_convergence_error.png\", bbox_inches='tight', dpi=1200.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GN_RL_for_MPC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
